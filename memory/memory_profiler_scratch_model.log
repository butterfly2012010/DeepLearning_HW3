Filename: LeNet5_from_scratch.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    87 2181.4102 MiB 2181.4102 MiB           1   @profile(precision=4, stream=open('./memory/memory_profiler_scratch_model.log','w+'))
    88                                         def train(model, train_dataloader, n_epochs):
    89                                             # EPOCHS = 5
    90 2181.4102 MiB   0.0000 MiB           1       train_losses = []
    91 2181.4102 MiB   0.0000 MiB           1       val_losses = []
    92 2181.4102 MiB   0.0000 MiB           1       train_accuracy = []
    93 2181.4102 MiB   0.0000 MiB           1       val_accuracy = []
    94                                         
    95 2206.1914 MiB  -2.4180 MiB           6       for i in range(n_epochs):
    96 2196.3945 MiB  -2.4180 MiB           5           print(f"epoch: {i+1}")
    97                                                 # train
    98 2196.3945 MiB  -2.4180 MiB           5           train_loss, train_acc = 0, 0
    99 2196.3945 MiB  -2.4180 MiB           5           with tqdm(total=train_dataloader.num_batches) as pbar:
   100 2212.7305 MiB -22048.7773 MiB        4955               for X_batch, Y_batch in train_dataloader:
   101                                                         # get batch, make onehot
   102                                                         # X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)
   103 2212.7305 MiB -22350.8477 MiB        4950                   Y_batch = MakeOneHot(Y_batch, D_out)
   104                                         
   105                                                         # forward, loss, backward, step
   106 2211.9961 MiB -27585.6562 MiB        4950                   Y_pred = model.forward(X_batch)
   107 2211.9961 MiB -22025.6055 MiB        4950                   loss, _ = criterion.get(Y_pred, Y_batch)  # loss, dout
   108 2211.9961 MiB -22037.2969 MiB        4950                   dout = Y_pred - Y_batch  # pred - label
   109 2212.6523 MiB -15673.9141 MiB        4950                   model.backward(dout)
   110 2212.6523 MiB -20957.2969 MiB        4950                   optim.step()
   111                                         
   112                                                         # train accuracy
   113 2212.6523 MiB -20951.0586 MiB        4950                   acc = acc_fn(Y_pred, Y_batch)
   114                                         
   115 2212.6523 MiB -20951.0586 MiB        4950                   train_loss += loss
   116 2212.6523 MiB -20951.0586 MiB        4950                   train_acc += acc
   117 2212.6523 MiB -20939.8711 MiB        4950                   pbar.update(1)
   118                                         
   119 2208.1211 MiB -70.2578 MiB           5           train_loss /= train_dataloader.num_batches
   120 2208.1211 MiB -22.3477 MiB           5           train_acc /= train_dataloader.num_batches
   121 2208.1211 MiB -22.3477 MiB           5           train_losses.append(train_loss)
   122 2208.1211 MiB -22.3477 MiB           5           train_accuracy.append(train_acc)
   123                                                         
   124 2208.1211 MiB -22.3477 MiB           5           if i % 1 == 0:
   125 2208.1211 MiB -22.3477 MiB           5               print("%s%% epoch: %s, train loss: %s" % (round(100*(i+1)/n_epochs, 4), i+1, round(train_loss, 4)))
   126                                         
   127                                         
   128                                                 # validation
   129 2208.1211 MiB -22.3477 MiB           5           val_loss, val_acc = 0, 0
   130 2210.2734 MiB -159.9609 MiB          45           for X_batch, Y_batch in val_dataloader:
   131                                                     # get batch, make onehot
   132                                                     # X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)
   133 2210.2734 MiB -118.5547 MiB          40               Y_batch = MakeOneHot(Y_batch, D_out)
   134                                         
   135                                                     # forward, loss, backward, step
   136 2209.6992 MiB -200.9531 MiB          40               Y_pred = model.forward(X_batch)
   137 2209.6992 MiB -175.2148 MiB          40               loss, _ = criterion.get(Y_pred, Y_batch)  # loss, dout
   138 2209.6992 MiB -175.2148 MiB          40               dout = Y_pred - Y_batch  # pred - label
   139 2210.0156 MiB -86.4414 MiB          40               model.backward(dout)
   140 2210.2734 MiB -117.5195 MiB          40               optim.step()
   141                                         
   142                                                     # train accuracy
   143 2210.2734 MiB -127.0977 MiB          40               acc = acc_fn(Y_pred, Y_batch)
   144                                         
   145 2210.2734 MiB -127.0977 MiB          40               val_loss += loss
   146 2210.2734 MiB -127.0977 MiB          40               val_acc += acc
   147                                         
   148 2206.1914 MiB -63.3828 MiB           5           val_loss /= val_dataloader.num_batches
   149 2206.1914 MiB  -2.4180 MiB           5           val_acc /= val_dataloader.num_batches
   150 2206.1914 MiB  -2.4180 MiB           5           val_losses.append(val_loss)
   151 2206.1914 MiB  -2.4180 MiB           5           val_accuracy.append(val_acc)
   152                                         
   153 2206.1914 MiB   0.0000 MiB           1       return train_losses, val_losses, train_accuracy, val_accuracy


